{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembler Methods\n",
    "\n",
    "An ensemble itself is a supervised learning algorithm. Ensemble learning systems are also called multiple classifier systems.\n",
    "Ensemble algorithms yield better results if there are significant differences or diversity among the models.\n",
    "\n",
    "[Graph](https://static-assets.codecademy.com/Paths/machine-learning-engineer-career-path/ensemble_methods/1866.jpg)\n",
    "\n",
    "Methods:\n",
    " * Bagging - Random Forests\n",
    " * Boosting - Adaptative Boosting, Gradient Boosting\n",
    " * Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55cb4ce",
   "metadata": {},
   "source": [
    "## Base\n",
    "\n",
    "In ensembling, often the base models that are chosen tend to underachieve on their own and are typically referred to as weak learners.\n",
    "Combining strong learners doesnâ€™t necessarily make the resultant ensembled model more performant so one might as well choose learners that cost less computationally.\n",
    "\n",
    "Weak Learner:\n",
    "Have high bias or high variance. The nature of the weakness of the base model is typically taken into consideration and is a design choice when determining the best ensembling method to use.\n",
    "\n",
    "The basic components of ensemble learning are:\n",
    "\n",
    "* Base models that are weak learners\n",
    "* An ensembling method that combines the base models to improve performance and robustness\n",
    "\n",
    "It is possible to have an ensemble model that performs worse than any one of its contributing base estimators. To circumvent this outcome it is important that the base estimators are `uncorrelated` and `independent`. Having a higher diversity among the trained base estimators leads to a stronger ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db080ccf",
   "metadata": {},
   "source": [
    "## Bagging - Bootstrap AGGregatING\n",
    "\n",
    "Weak learners as base models that are `complex` and tend to suffer from `high variance`.\n",
    "\n",
    "Bootstrapping and aggregation. Bagging can be used for both `classification` and `regression` problems.\n",
    "\n",
    "Bagging is a learning technique that is done in parallel. Each of the base models is trained independently of the others. Additionally, each base model is trained using only a `subset` of the original features. This allows them to be diverse from one another, often leading to a very strong ensemble model when aggregated.\n",
    "\n",
    "Base models are `decision trees` that are relatively large and overfit to the bootstrapped subset of data provided to each of them.\n",
    "\n",
    "Steps:\n",
    "Once each of the base models is trained, the method for ensembling tends to be a simple aggregation technique over each of the models.\n",
    "A majority vote for classification problems and averaging for regression problems.\n",
    "\n",
    "A common implementation of a bagging algorithm that uses decision trees as their base model is the `Random Forest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9f7b6",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Weak learners are too `simple` and tend to suffer from `high bias`.\n",
    "\n",
    "Boosting is a sequential learning technique where each of the base models builds off the previous model. Each subsequent model aims to improve the performance of the final ensembled model by attempting to fix the errors in the previous stage.\n",
    "\n",
    "Two common implementations of the boosting algorithm are `Adaptive Boosting` and `Gradient Boosting`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3eb34",
   "metadata": {},
   "source": [
    "![image.png](https://static-assets.codecademy.com/Paths/machine-learning-engineer-career-path/ensemble_methods/1866.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe2d12d",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Flexible ensembling technique where a final model is trained to learn how to best combine a set of base models to make strong predictions.\n",
    "In contrast to bagging and boosting, the base models in stacking do `not need to be the same type` of learning algorithm.\n",
    "\n",
    "While bagging and boosting are built with base models that are weak learners, that does not necessarily have to be the case for stacking. A stacking algorithm can be used to combine decently performing learners as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
